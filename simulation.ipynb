{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SingularityNET Simulation\n",
    "\n",
    "This is a simulation of the SingularityNET itself.\n",
    "Its purpose is to compare solution approaches to problems in the network, as well as to make solutions out of AI services, which is the function of the SingularityNET. \n",
    "It facilitates algorithms that gain experience in tailoring solutions to individual needs, through agent based co-evolution, and helps users to test these algorithms before using them in the real world SingularityNET.\n",
    "It helps AIs to build AIs.\n",
    "This notebook offers an initial look at the fundamental problem of how to construct AI software solutions automatically out of AI programs submitted to the SingularityNET.\n",
    "\n",
    "The simulation tests how different solutions affect SingularityNET values, such as delivering maximum utility to people seeking AI software at the lowest cost, giving everyone who makes high quality software a chance to sell, distributing credit where credit is due, and complexification to facilitate a singularity.\n",
    "An important value of the SingularityNET is community participation in its construction, thus we offer this simulation in the competitive arena software of OpenAI to attract the community to play the \"Singularity\" game, in the spirit of democratic meritocracy.\n",
    "\n",
    "This simulation is implemented in the [Mesa](https://github.com/projectmesa/mesa) agent based software in a style that mimics [OpenAI Gym](https://gym.openai.com/) reinforcement learning competition software, except that its more tailored to multiple agents.\n",
    "Mesa handles the network of multiple agents, their rules of interaction, and their schedule and cooperative setups.\n",
    "Mesa simulation agents read and write to a network-mediated blackboard.\n",
    "The blackboard displays human requests for a particular algorithm type that can pass a test (that has a gradient) on a particular dataset for a particular price range in AGI tokens.\n",
    "The algorithm type, test, and dataset are from an ontology described in a json configuration file that describes preregistered software.\n",
    "In response, agents can place on the blackboard offers to construct, sell, or buy software from other agents. \n",
    "Construction can include finding appropriate types from the human-designed ontology and parameter settings in combinations that fit particular use cases. \n",
    "However, it can also include inventing new algorithms that trade an input list for an output list (in [OfferNet](https://singnet.github.io/offernet/public/offernet-documentation/index.html) fashion) that are not listed in the human-designed ontology.  \n",
    "Agents may also display a sign in the form of a vector of floats, and may express their preferences for agents whose sign is closest to an ideal sign. \n",
    "The sign is used by the simulation to rank agents that already have matching trade plans.\n",
    "\n",
    "The simulation has a general, flexible knowledge representation that is meant to accommodate the needs of different machine learning and reinforcement learning algorithms.  \n",
    "Different modular machine/reinforcement learning algorithms may compete on the same blackboard in contests to see which algorithms best fill user, developer, and AI growth needs. \n",
    "Alternatively, the simulation could focus on a single algorithm, measuring its effects in isolation, in a more cooperative scenario.  \n",
    "The different algorithms implement agent integration schemes with their own approaches.\n",
    "For example, it is up to each machine learning/reinforcement learning algorithm to assign meanings to the float vector signs that they read and display on the blackboard. \n",
    "Different possible interpretations of the sign's float dimensions include reputation scores, the identity of an agent, the information needed for OfferNet trades, an emergent agent language or an emergent software ontology that augments the human designed ontology.  \n",
    "Agents may also require that software pass certain tests on data, seen and unseen, before a transaction is accepted; they can list any number of tests and datasets.  \n",
    "They have the choice to indicate the type of software they are buying, selling, or constructing with the human ontology, which indicates an input and output list for them. \n",
    "Buying agents (and humans) can use any level of generality of the ontology in their specification of what software they want.  \n",
    "Alternatively they may offer an input list for an output list and allow the sign field to represent the ontological type.  \n",
    "They have the choice of paying solely through the input and output list without using AGI tokens for currency, in order to implement an OfferNet type of scenario - although they have the option of AGI tokens, they may not maximize these as it may not be in their utility or fitness function to do so. \n",
    "During competition, different meanings of signs could either isolate agents into solution groups having the same algorithm, or algorithms groups could learn other group's meanings, creating a competition on the meaning of signs.\n",
    "\n",
    "Once all offers are on the blackboard, the blackboard makes the matches ranked by the cosine distance of those signs that have overlapping prices ranges and exceed agents stated test threshold criteria.\n",
    "The environment creates the software, and distributes funds.  \n",
    "It notifies the agent of payment through an OpenAI Gym-like environment reward signal, detailing the particular float vectors that were rewarded, and giving the agent a chance to view the blackboard.  \n",
    "However it is up to the particular reinforcement learning algorithm to choose what aspects of the environment reward, observation, and of itself to include in its personal reward.     \n",
    "Mesa keeps track of agent and human utility satisfaction and prices, and can be made to measure SingularityNET values such as adequate exploration of unknown agent capabilities.  \n",
    "\n",
    "This general design promotes SingularityNET values in several ways. \n",
    "The simulation allows credit to be assigned through the market price signal.  \n",
    "It can serve as the baseline for comparison with other assignment of credit algorithms, one that holds all else the same.   \n",
    "In this simulation, agents can also gain through more OfferNet-type utility based trades (for example, use of the input list for training), or more reputation utility based trade (for example, volunteering to increase ones reputation, to ultimately increase its market value in AGI tokens). \n",
    "Correct assignment of credit is needed to promote quality software that satisfies the user, while at the same time assures that developers have a fair shot.  \n",
    "It is expected that agents that learn to do jobs of uniquely higher quality in uniquely necessary areas will be able to charge higher prices.  \n",
    "Since agents are stateful and able to gain experience from a variety of different problems, it is expected that one way for an agent to gain a higher price would be to reuse and accumulate knowledge in one area, that is, to specialize.  \n",
    "Such agents would contain parameters and functions suited to particular types of problems.  \n",
    "Importantly, these problems need not be expressed in the human designed ontology: they may involve whatever agent roles are needed in practice to get the job done.  \n",
    "The emergence of automated new constructions and new ontological categories is prerequisite to AIs which can construct themselves and thus supports a singularity.   \n",
    "As agents learn how to buy and sell the services of other agents, particularly higher quality but cheaper agents that they have learned to recognize through sign and test, it is expected that curating agents will emerge.  \n",
    "Curation agents that can both recommend software successfully and give new developers a chance to become known satisfy both user and developer. \n",
    "The \"no free lunch\" theorem of Machine Learning and Optimization - the idea that no one solution fits all- makes curation agents essential to the SingularityNET.  \n",
    "\n",
    "In creating this environment we have had to invent a few things that are also useful in other contexts.  \n",
    "The effort to create a gradient of python programs for machine learning programs to construct useful solutions, through representation and distributed AI, is the first contribution.  \n",
    "A second contribution is an agent economy that solves problems.\n",
    "Finally, we designed a convenient way of storing the partial products of AI solutions so the same combination, whether partial or complete, never has to be computed twice, even in subsequent runs of the simulation.\n",
    "\n",
    "All of this may seem confusing. \n",
    "Is it a simulation?  \n",
    "Or does it build products?  \n",
    "Well, both, as we will explain in scaffolded fashion in this notebook.  \n",
    "In part one, we introduce the knowledge representation of the communicating agents on the blackboard, as well as the utilities that make combinatorial AI practical.  \n",
    "In part two, we will demonstrate an initial use of the simulation with a particular reinforcement learning algorithm to construct the AI software: SISTER (Symbolic Interactionist Simulation of Trade and Emergent Roles). \n",
    "SISTER is an evolutionary algorithm that implements agent-based coevolution.  \n",
    "It uses the sign areas of the blackboard communications to learn specialized, non-preprogrammed roles for the agents, needed to solve problems.  \n",
    "Utility/fitness/objective function for individual agents is measured in AGI tokens, making use of Adam Smith's invisible hand rather than group selection coevolution methods that base utility on the good of the whole.  \n",
    "Assignment of credit through the market process is inherit in this method.   \n",
    "As her name suggests, emergent roles in SISTER are macro institutions from the micro-macro integration processes of micro-economics and micro-sociological symbolic interactionism.  \n",
    "Thus SISTER models growth in AI on growth in economy and society. \n",
    "This appreciation for the life of the market does not imply that the author believes nature should be unfettered, rather, that we must first understand both nature and natural illness before we are able to apply technology to the treatment of today's policy challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.  Representation for Evolvability\n",
    "\n",
    "This simulation uses a representation of Python that addresses problems of wastefulness in combinatorial search techniques such as evolutionary algorithms, while at the same time making it easier to evolve AI solutions composed of existing python programs.  \n",
    "Since Python is not a tree based language, it is more difficult to evolve than tree based, functional languages such as Scala, and especially those that can easily use their program representations as data, such as Scheme.  \n",
    "However, since it is the most used language in data science, we want to be able to use building blocks from existing Python programs. \n",
    "Many data science programs, such as tensorflow, are arranged in such building blocks.  \n",
    "Python has a few functional programming representations, such as lamdas for unnamed functions, but it does not offer currying, so we make use of an external implementation of it.\n",
    "The pickling of bound curries in this preliminary implementation does not make use of introspection or marshalling, which would be important in making curries that were mobile across machines.  \n",
    "Since pickles can be malicious, it is recommended that security issues be handled before this prototype simulation is actually used in competitions.  \n",
    "However, when used for a simulation on a single machine, these curried pickles are saved to disk and kept track of to ensure that nothing need be computed more than once. \n",
    "Once security issues are resolved, the curry representation in itself is good for dividing a problem up into parts on multiple machines for high performance computing, and serialization in itself is good for checkpoint storage of computationally intense functions as are many machine learning functions.  \n",
    "\n",
    "Tree based genetic programming - the standard in evolving computer programs- has difficulty converging in ways that don't grow too quickly.  \n",
    "Not being able to converge is problematic in agents that base their interactions on the equilibria of economics: for these agents, convergence represents a solution, a dynamic compromise, and a software institution \"fuzzy rule\". \n",
    "SISTER, for example, depends upon the compromise of convergence between agents to develop the institutions of role and role relations that are the solutions to AI integration problems. \n",
    "Trees offer the ability for different modifiers to refer to the same entity without having to rename it, a great advantage in genetic programming (GP).  \n",
    "However, GP programs have difficulty in converging, because growing trees often involve displacing insertions, with small changes in genotype resulting in large changes in phenotype. \n",
    "That is why we combine genetic programming with agents that can self-organize into subroutines, so that we take advantage of referring to the same entity as trees do, but so that multiple branches of a tree (forks) are implemented with multiple agents.  \n",
    "In our curried representation, a position corresponds to itself in the next generation, facilitating convergence. \n",
    "However, chromosome size can change as is needed for growth, not by insertion but using markers. \n",
    "Using genetic terminology, by using a stop codon (marker) that moves through introns that lengthens a chromosome but preserves position.  \n",
    "The hierarchical tree structure of the curries also gives gradient to machine learning programs by defining what is a distant/close change, becoming a kind of [Gray coding](https://en.wikipedia.org/wiki/Gray_code) that makes changes that are close in phenotype, close in genotype, as well as changes that are far in phenotype, far in genotype.\n",
    "The representation using an ontology with markers to give gradient is a contribution of this project that allows many more functions to be represented than in standard genetic programming.\n",
    "\n",
    "The knowledge representation of the agent communications is a vector of floats sent to the blackboard environment as a message, corresponding to an OpenAI Gym action. \n",
    "A floating point representation was chosen because of its generality, and can be used or converted into use by any machine learning algorithm.  \n",
    "Such representation allows a more exacting search in the float parameter space quite easy to add on at a later point. \n",
    "It uses the ontology representation, and likewise is designed for evolvability by facilitating convergence and gradient, needed by many machine learning algorithms.  \n",
    "For example, the SISTER algorithm uses the float vector of agent communications as the chromosome inside of an individual [CMA-ES](https://en.wikipedia.org/wiki/CMA-ES) algorithm within each agent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration file\n",
    "The configuration file contains parameters, the initial agent configuration of messages on the blackboard, and the ontology.  \n",
    "First we show a blackboard message, a human request to buy a clusterer. \n",
    "It shows the price range (between low and high) he or she is willing to pay, the tests the clusterer must pass, and the performance threshold (we use a short data file for this example to make a quick demo, so we have turned off the threshold). \n",
    "It also shows a vector of floats as a sign to rank potential selling agents, based on the closeness of their displayed sign to this vector of floats, given that they meet all of the other conditions of the trade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we import the configuration file and show the first entry in the blackboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"Human\",\n",
      "  \"label\": \"Cluster Seeking Human\",\n",
      "  \"sign\": [\n",
      "    0.45,\n",
      "    0.23,\n",
      "    0.94,\n",
      "    0.24,\n",
      "    0.68,\n",
      "    0.29,\n",
      "    0.95,\n",
      "    0.47\n",
      "  ],\n",
      "  \"trades\": [\n",
      "    {\n",
      "      \"type\": \"buy\",\n",
      "      \"sign\": [\n",
      "        0.83,\n",
      "        0.59,\n",
      "        0.35,\n",
      "        0.7,\n",
      "        0.13,\n",
      "        0.93,\n",
      "        0.35,\n",
      "        0.12\n",
      "      ],\n",
      "      \"item\": \"clusterer_stop\",\n",
      "      \"low\": 0.0,\n",
      "      \"high\": 0.8,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"test_clusterer_silhouette\",\n",
      "          \"data\": \"data_freetext_internetResearchAgency\",\n",
      "          \"threshold\": -0.99,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('config.json') as json_file:  \n",
    "    config = json.load(json_file)\n",
    "    print(json.dumps(config['blackboard'][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This offer to buy matches best with the following offer to sell, also in the blackboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"SISTER\",\n",
      "  \"label\": \"Clusterer that purchases vector space Agent 1\",\n",
      "  \"sign\": [\n",
      "    0.86,\n",
      "    0.67,\n",
      "    0.3,\n",
      "    0.73,\n",
      "    0.1,\n",
      "    0.96,\n",
      "    0.29,\n",
      "    0.19\n",
      "  ],\n",
      "  \"trades\": [\n",
      "    {\n",
      "      \"type\": \"sell\",\n",
      "      \"sign\": [\n",
      "        0.45,\n",
      "        0.38,\n",
      "        0.96,\n",
      "        0.38,\n",
      "        0.64,\n",
      "        0.96,\n",
      "        0.74,\n",
      "        0.57\n",
      "      ],\n",
      "      \"item\": \"clusterer_sklearn_kmeans_20clusters\",\n",
      "      \"low\": 0.7,\n",
      "      \"high\": 0.99,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"stop_clusterer_silhouette\",\n",
      "          \"data\": \"data_freetext_internetResearchAgency\",\n",
      "          \"threshold\": 0.4,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"buy\",\n",
      "      \"sign\": [\n",
      "        0.45,\n",
      "        0.89,\n",
      "        0.85,\n",
      "        0.3,\n",
      "        0.59,\n",
      "        0.45,\n",
      "        0.58,\n",
      "        0.38\n",
      "      ],\n",
      "      \"item\": \"vectorSpace_stop\",\n",
      "      \"low\": 0.45,\n",
      "      \"high\": 0.99,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"test_stop_silhouette\",\n",
      "          \"data\": \"data_freetext_internetResearchAgency\",\n",
      "          \"threshold\": 0.77,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"stop\",\n",
      "      \"sign\": [\n",
      "        0.83,\n",
      "        0.59,\n",
      "        0.35,\n",
      "        0.7,\n",
      "        0.13,\n",
      "        0.93,\n",
      "        0.35,\n",
      "        0.12\n",
      "      ],\n",
      "      \"item\": \"data_stop\",\n",
      "      \"low\": 0.0,\n",
      "      \"high\": 0.54,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"test_clusterer_stop\",\n",
      "          \"data\": \"data_freetext_internetResearchAgency\",\n",
      "          \"threshold\": 0.77,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(config['blackboard'][1], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent below has parameterized the clusterer and bought a vector space, so as to sell it to the Human cluster seeker.\n",
    "He has matching trade plans with both the human and with the vector space seller.  The vector space seller has a longer program, perhaps as we would see in an agent with a lot of experience.\n",
    "Many of the individual programs would work on their own, but the addition of more make them work better, giving his offer gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"SISTER\",\n",
      "  \"label\": \"NLP pipeline vector specialist, Agent 2\",\n",
      "  \"sign\": [\n",
      "    0.42,\n",
      "    0.99,\n",
      "    0.75,\n",
      "    0.31,\n",
      "    0.55,\n",
      "    0.48,\n",
      "    0.53,\n",
      "    0.33\n",
      "  ],\n",
      "  \"trades\": [\n",
      "    {\n",
      "      \"type\": \"sell\",\n",
      "      \"sign\": [\n",
      "        0.45,\n",
      "        0.89,\n",
      "        0.85,\n",
      "        0.3,\n",
      "        0.59,\n",
      "        0.45,\n",
      "        0.58,\n",
      "        0.38\n",
      "      ],\n",
      "      \"item\": \"vectorSpace_gensim_doc2vec_200size_1000iterations_5minFreq\",\n",
      "      \"low\": 0.0,\n",
      "      \"high\": 0.5,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"test_stop_silhouette\",\n",
      "          \"data\": \"data_freetext_internetResearchAgency\",\n",
      "          \"threshold\": 0.77,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"construct\",\n",
      "      \"sign\": [\n",
      "        0.45,\n",
      "        0.59,\n",
      "        0.45,\n",
      "        0.35,\n",
      "        0.64,\n",
      "        0.67,\n",
      "        0.28,\n",
      "        0.75\n",
      "      ],\n",
      "      \"item\": \"preprocessor_freetext_tag\",\n",
      "      \"low\": 0.0,\n",
      "      \"high\": 0.4,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"test_clusterer_stop\",\n",
      "          \"data\": \"data_freetext_internetResearchAgency\",\n",
      "          \"threshold\": 0.77,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"construct\",\n",
      "      \"sign\": [\n",
      "        0.67,\n",
      "        0.49,\n",
      "        0.28,\n",
      "        0.88,\n",
      "        0.19,\n",
      "        0.32,\n",
      "        0.89,\n",
      "        0.22\n",
      "      ],\n",
      "      \"item\": \"preprocessor_freetext_lemmatization\",\n",
      "      \"low\": 0.0,\n",
      "      \"high\": 0.4,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"test_clusterer_silhouette\",\n",
      "          \"data\": \"stop_freetext_internetResearchAgency\",\n",
      "          \"threshold\": 0.77,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"construct\",\n",
      "      \"sign\": [\n",
      "        0.97,\n",
      "        0.53,\n",
      "        0.68,\n",
      "        0.03,\n",
      "        0.56,\n",
      "        0.32,\n",
      "        0.39,\n",
      "        0.45\n",
      "      ],\n",
      "      \"item\": \"preprocessor_freetext_strip\",\n",
      "      \"low\": 0.0,\n",
      "      \"high\": 0.4,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"test_stop_silhouette\",\n",
      "          \"data\": \"data_freetext_internetResearchAgency\",\n",
      "          \"threshold\": 0.77,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"construct\",\n",
      "      \"sign\": [\n",
      "        0.83,\n",
      "        0.59,\n",
      "        0.35,\n",
      "        0.7,\n",
      "        0.13,\n",
      "        0.93,\n",
      "        0.35,\n",
      "        0.12\n",
      "      ],\n",
      "      \"item\": \"preprocessor_freetext_shuffle\",\n",
      "      \"low\": 0.0,\n",
      "      \"high\": 0.4,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"test_clusterer_silhouette\",\n",
      "          \"data\": \"data_stop_internetResearchAgency\",\n",
      "          \"threshold\": 0.77,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"stop\",\n",
      "      \"sign\": [\n",
      "        0.45,\n",
      "        0.44,\n",
      "        0.98,\n",
      "        0.43,\n",
      "        0.93,\n",
      "        0.53,\n",
      "        0.27,\n",
      "        0.16\n",
      "      ],\n",
      "      \"item\": \"stop\",\n",
      "      \"low\": 0.0,\n",
      "      \"high\": 0.4,\n",
      "      \"tests\": [\n",
      "        {\n",
      "          \"test\": \"stop_clusterer_silhouette\",\n",
      "          \"data\": \"data_freetext_internetResearchAgency\",\n",
      "          \"threshold\": 0.77,\n",
      "          \"hidden\": false\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(config['blackboard'][2], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation parameters in the config file are used to interpret a vector of floats, which is what the machine learning agents that are not stubbed put on the blackboard.\n",
    "This scenario has five such agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"label\": \"Cluster Scenario\",\n",
      "  \"output_path\": \"competing_clusterers/\",\n",
      "  \"sign_size\": 8,\n",
      "  \"num_trade_plans\": 10,\n",
      "  \"item_size\": 8,\n",
      "  \"num_tests\": 5,\n",
      "  \"min_token_price\": 1,\n",
      "  \"max_token_price\": 100,\n",
      "  \"max_iterations\": 10,\n",
      "  \"agent_parameters\": {\n",
      "    \"SISTER\": {\n",
      "      \"num_chromosomes\": 100,\n",
      "      \"num_chromosomes_kept\": 50\n",
      "    },\n",
      "    \"Human\": {}\n",
      "  },\n",
      "  \"chance_of_stop_codon\": 0.1,\n",
      "  \"iterative_convergence threshold\": 100,\n",
      "  \"random_agents\": {\n",
      "    \"SISTER\": 5\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(config['parameters'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blackboard knowledge representation for agent communications, a vector of floats from zero to one is as follows:\n",
    "\n",
    "sign_to_display \\[float\\*sign_size\\]\n",
    "\n",
    "num_blocks \\*\\(\n",
    "\n",
    "type\\[1 float: construct, buy , sell, or stop\\] \n",
    "\n",
    "item \\[float\\*item_size: interpretaton comes from ontology\\] (Uses stop codons, to generalize)\n",
    "\n",
    "test \\[num_tests\\* (float\\*item_size for the test program, float\\* item_size for the data to test, 1 float for threshold,  1 float for boolean is_hidden)\\] (Uses stop codons) \n",
    "\n",
    "low (float: lower price interpreted with min_amount and max_amount)\n",
    "\n",
    "high  (float: higher price interpreted with min_amount and max_amount)\n",
    "sign_to_seek(float\\*sign_size)\n",
    "\n",
    "\\)\n",
    "\n",
    "The sign to display on the blackboard is a vector of floats (8 in the examples above), which is compared to the sign-to-seek field of someone seeking out the agent for an offer.\n",
    "The cosine distance ranks agents according to how closely they resemble the ideal agent, and can be used in a variety of algorithms.\n",
    "\n",
    "Each block represents a different trade offer or construction notice to put on the blackboard.\n",
    "An agent can communicate up to num_blocks blocks, but the number of actual communications is controlled by the stop codon.\n",
    "An agent may indicate an item to sell, buy or construct.  \n",
    "\n",
    "The human-designed ontology includes an input list and output list in \\*args, \\*\\*kwargs python syntax.\n",
    "Input and output lists are important for determining the arity for assignment in the genetic programming [GEP (Karva)](https://www.gene-expression-programming.com/Tutorial002.asp) representation, and also for calculating the input and output for self-organized functions made by the agents in the course of their transactions.\n",
    "Although it is not implementd now, a check could be made for input output compatablility (now they just error out).\n",
    "For example, the agents could conceivably invent a new clusterer that wasn't one of the listed ones, in which case they may indicate a clusterer using the human-designed general category with a stop codon immediately following, and then have the input and output list calculated from the Karva ordering.\n",
    "Because the generated input and output list is used with software references, it only needs the \\*args syntax part of the ontology.\n",
    "\n",
    "The tests are optional to the agent and used in a buy block to require that software passes with a threshold before it is accepted, with or without revealing the test and data to pass (so that it cant be gamed or trained on). \n",
    "At this momment hidden price ranges and tests are not implemtented yet.\n",
    "In the future, the same tests in the construct and the sell block indicate that the agent has passed the test, before the item was put on the board, and to have this hidden is a note to the agents self to pass the test before it is put on the board.\n",
    "Agents do not need such tests in that they could rely on reputation or market price alone to incentivize themselves to quality, but such tests are good points of quality control, and necessary when dealing directly with human beings in establishing criteria for transaction validation. \n",
    "\n",
    "The above messages are from initialized , stubbed agents that give an example of a sucessful set of trade plans.\n",
    "There can also be randomized agents, whose trade plans are generated from a random vector of floats, which can be processed by algorithms like SISTER/CMS-ES.\n",
    "We show the translation process below.\n",
    "Now we load the simulation, and run it through several trades.\n",
    "Then, we will generate a float, and send it to the part that interprets the float into a message.\n",
    "If you re-run (`SHIFT+ENTER`) the cell many times, you will see many examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'blackboard_agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-04baedc02c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#first find out the size of the vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msnetsim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnetSim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msnetsim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repositories/simulation/simulation/SnetSim.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, study_path)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0magent_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'blackboard_agents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'blackboard_agents'"
     ]
    }
   ],
   "source": [
    "from simulation.SnetSim import SnetSim\n",
    "\n",
    "#first find out the size of the vector\n",
    "snetsim = SnetSim('config.json')\n",
    "snetsim.go()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent = snetsim.schedule.agents[0]\n",
    "print('The number of floats in a message vector is: '+str(agent.vector_size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "floatvec = [random.uniform(0, 1) for i in range(agent.vector_size())] \n",
    "\n",
    "message = agent.float_vec_to_trade_plan(floatvec)\n",
    "print(json.dumps(message, indent=2))\n",
    "print(\"comes from this vector:\")\n",
    "print(floatvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Human-Designed Ontology\n",
    "\n",
    "The human-designed ontology is designated in the config file, a JSON file. We say \"human designed\" because when agents buy and sell to each other, their constructions from other programs are themselves programs. The official SingularityNET ontology or API of APIs would contain all of the software entered into the singularity net, but this possibly smaller representation is used solely for evolution, which may read from the official ontology and only use a subset of the available programs. Different instances of the ontology used in evolution may be created for individual problems, so that the functions that are made available to use in a solution can be reduced to those that are likely to be in that solution over a threshold, so that those functions may be weighted by their likelihood to be in a solution, and so that different parameter values to explore together may be indicated. All these statistical relations between functions and particular solutions can be learned from current solutions in the open source, as in the Microsoft DeepCoder project (https://openreview.net/pdf?id=ByldLrqlx).  \n",
    "\n",
    "A small subsection of  ontology follows.  Its hierarchical structure is used to interpret the meaning of ontology items, explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('config.json') as json_file:  \n",
    "    config = json.load(json_file)\n",
    "    print(json.dumps(config['ontology']['clusterer']['nltk'], indent=2))\n",
    "   # for type in config['ontology']:\n",
    "        #print('Name: ' + type['name'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical representation in the ontology has consistent levels, from root to branch, of function type, datatype or brand, algorithm, parameter1,parmeter 2 ,parameter3, etc. As consistancy is important for evolvability, care is taken in creating the JSON file to list parmeters in a consistent order, for example the number of clusters in clustering algorithms might all be listed first. The siblings within a level are also listed in a meaningful order, for example all clustering algorithms might should be sampled at 10 clusters, 30 clusters and 50 clusters.  Some paths from root to branch in the example ontology are test-clusterer-silouhette, data-freetext-internetResearchAgency, and vectorSpace-gensim-doc2vec-50size-200iterations-2minFrq.   50size, 200iterations, and 2minFrq are three parameter values that our ontology designates should be tested together in gensim's doc2vec algorithm, a vectorSpace algorithm. Admitedly there are many more possible combinations of parameters that can be explored than can be explicated individually in a JSON file, but these are the ones that are worth saving to disk as checkpoints. Algorithms can still be used to zero in on exact float values and parmeter relations, but we would not save all combinations of these parmeter values to disk.   Because we are dealing with curried functions, a function with one of the parmeters bound is itself a function.  From root to branch we go from more general to more and more specific functions, until when all values are bound, the output of the function only differs if it is stochastic.  So as we go from left to right , we go from broader to narrower posiblities.  \n",
    "\n",
    "Internal variables as opposed to the next level of the tree start with an underscore, and inherit values from parent nodes if they are not specified in a child node.  The internal variables at each level include those we would expect in an api, the types of the input and output, in key word arguments as well as arguments.  In the ontology file, the input values that are not bound are the ones expected from other agents. If two or more inputs are not bound, then the input of two or more agents is required. Although more than one output is allowed in python, they are not in this protptype curried representation. The weight internal variable is normalized with its siblings, and represents the probability that that a sibling node occurs in a solution given the parent node. It can be filled in with data from techniques that use the likelihood of function use given the problem such as Microsoft's DeepCoder. Right now, however, the liklihood of any child is the same.  \n",
    "\n",
    "The hierarchical design of functions gives gradient to the vector of floats representation of an item (to buy, sell, or construct)  in the agent communication to the blackboard. Each float in the float vector representation of an item represents a level in the hierarchy, with the values of floats to the left determining the meanings of floats to the right.  The first float in this hierarchy, is whether the node is a test, or data, or preprocessor, or vectorSpace or clusterer.  Since in this case, they are all equally weighted, each has a 20% chance of being picked. If, say, the float in position 1 fell in the range of clusterer, of values .8 and above, then floats above .5 in position 2 might indicate the brand NLTK, since there are two brands.  Since the string ends with a stop codon on the left, and more specific answers occur on the left, a consensus about general facts about the item may form, or converge, before the specific facts about an item.  This representation works with gene switches which can be disruptive, but the consistency of the meanings of the levels helps to mitigate disruptions to the left.   For example, a 300 size vector would mean the same thing in a vector space whether the brand was NLTK or scikit learn.  \n",
    "\n",
    "Now we are in a position to interpret agent communication on the board.  First we look at a scenario of individual float communications on the blackboard, interpreting each float and how they construct a program. Next we take the steps to construct a program from those same floats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Interpretation of the vector of floats that agents communicate with on the blackboard\n",
    "\n",
    "\n",
    "The blackboard is intialized with five offers made by three humans.  One of the humans buys a test and data from other humans, and asks singularity net to construct some software, a clusterer of the Internet Research Agency tweets, and to add a column with cluster distance to a dataframe.\n",
    "\n",
    "In interpreting the vector of floats on the blackboard remember that, except for the signs, each float from 0 - 1 is divided up by the probability of a value.  0.37 is interpreted as a sell because the possible values (alleles), buy, sell, construct, and stop are evenly distributed, and 0.37 is in the sell range from 25 to 49.  In the ontology, the allele probabilies are weighted and normalized.\n",
    "\n",
    "___\n",
    "_____\n",
    "\n",
    "0000000 .4 sign reserved for human\n",
    "\n",
    "0.37 sell\n",
    "\n",
    "0.22 data  0.4 freetext 0.17 internetResearchAgency .99 stop ... points to data in the api of apis\n",
    "\n",
    ".87...  test type not chosen \n",
    "\n",
    "0.11 0.27 accepts between 11 and 27 agiTokens (hidden)\n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "\n",
    "0000000 .3 sign reserved for human\n",
    "\n",
    "0.58 sell\n",
    "\n",
    "0.18 test  0.4 clusterer 0.88 silhouette .99 stop ...  points to test in api of apis\n",
    "\n",
    ".59...  test type not chosen \n",
    "\n",
    "0.34 0.45 accepts between 34 and 45 agiTokens (hidden)\n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "0000000 .2 sign reserved for human\n",
    "\n",
    "0.18 buy\n",
    "\n",
    "0.22 data  0.4 freetext 0.17 internetResearchAgency .99 stop ...  points to data in the api of apis\n",
    "\n",
    ".24... test type not chosen \n",
    "\n",
    "0.27  0.76 accepts between 25 and 76 agiTokens  \n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "0.08 buy\n",
    "\n",
    "0.18 test  0.4 clusterer 0.88 silhouette  .99 stop ....  points to test in api of apis\n",
    "\n",
    ".78 ...test type not chosen \n",
    "\n",
    "0.34 0.41 accepts between 34 and 41 agiTokens \n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "0.08 buy\n",
    "\n",
    "0.65 clusterer .99 stop ...\n",
    "\n",
    "(the cluster it buys must take freetext in a dataframe in the column named text and output a dataframe of numbers in the field named cluster)\n",
    "\n",
    ".18 test  0.4 clusterer 0.88 silhouette .99 stop ....  0.22 data  0.4 freetext 0.17 internetResearchAgency  . .99 stop ....\n",
    "0.54 threshold .67 hidden .93 stop\n",
    "\n",
    "0.34 0.41 accepts between 24 and 34 agiTokens \n",
    "\n",
    "00000000 sign not used by human\n",
    "\n",
    "_____\n",
    "_____\n",
    "\n",
    "Two constructing agents are minimal for this problem because of the two data streams coming into the silouhette test.  Here is the communication of the first automated agent:\n",
    "\n",
    "_____\n",
    "\n",
    ".68 .2 .34 .52 .31 .95 .28 .46 sign displayed\n",
    "\n",
    "0.08 construct\n",
    "\n",
    "0.7 clusterer  0.43 sklearn 0.13 kmeans .80 20clusters .32 ... (stop not necesary if there are no parameters left)\n",
    "\n",
    ".28 ... test type not chosen \n",
    "\n",
    "0.1 0.4 accepts between 10 and 40 agiTokens \n",
    "\n",
    ".27 .85 .03 .24 .95 .12 .37 .75 sign sought\n",
    "\n",
    "0.21 buy vectorSpace .99stop ...\n",
    "\n",
    ".76 ...test type not chosen \n",
    "\n",
    "0.34 0.41 accepts between 34 and 41 agiTokens \n",
    "\n",
    ".33 75 .94 .476 .06 .26 .84 .35 sign sought\n",
    "\n",
    ".93 stop ...\n",
    "_____\n",
    "\n",
    "This representation is not hard to evolve because the only hard guesses are the vectorSpace purchase, the clusterer, and the construct while the rest have a smooth gradient. A rich ecosystem of problems on the blackboard that include other vector spaces besides the one ordered here would help to make this agent even easier to evolve, because these may be more. Because the cluster takes in two inputs, at most only two more construct or buy blocks are allowed, and then there is an automatic stop. In this case the stop evolved anyway, so that the bought item is used twice.\n",
    "\n",
    "\n",
    "Transactions only go through if all pieces are present, but assuming that will happen, this is the translation of the code that the human has purchased so far:\n",
    "\n",
    "_____\n",
    "\n",
    "blackboard['test_clusterer_silouhette']\n",
    "\n",
    "(blackboard['clusterer_sklearn_kmeans_20clusters']\n",
    "\n",
    "(vectorSpace\n",
    "\n",
    ")) \n",
    "\n",
    "(vectorSpace)\n",
    "\n",
    "_____\n",
    "\n",
    "Each entry in the blackboard dictionary is the curry corresponding to its name\n",
    "Since there is a fork, another agent is needed to create the vector space. \n",
    "We use only one to demonstrate a minimal agent setup.  It is harder to evolve because it involves the data guess, the vector guess, and multiple construction guesses. However, the multiple construction of preprocessors have gradient because the program will still work and get a gradient answer from test.  The use of the sign field by communicating agents would make it easier still to evolve, as will be demonstrated in our agent based coevolution section.  \n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    ".63 .52 .54 .82 .91 .05 .22 .57 sign displayed\n",
    "\n",
    "0.08 construct\n",
    "\n",
    "0.73 vectorSpace  0.84 doc2vec 0.11 gensim .88 size200 .79 iterations1000  .62 minfreq5  .99 stop \n",
    "\n",
    ".28 ... test type not chosen \n",
    "\n",
    "0.34 0.41 accepts between 34 and 41 agiTokens \n",
    "\n",
    ".53 .25 .34 .46 .76 .22 .81 .75 sign sought\n",
    "\n",
    "0.18 construct\n",
    "\n",
    "0.40 preprocessor  0.23 freetext 0.13 emojiRemoval .20 ...\n",
    "\n",
    ".28 ...test type not chosen \n",
    "\n",
    ".90 stop...\n",
    "\n",
    ".93 stop...\n",
    "\n",
    "0.13 construct\n",
    "\n",
    "0.43 preprocessor  0.46 freetext 0.13 lemmatization .51 ...\n",
    "\n",
    ".28 ...test type not chosen \n",
    "\n",
    ".96 stop...\n",
    "\n",
    ".96 stop...\n",
    "\n",
    "0.03 construct\n",
    "\n",
    "0.39 preprocessor  0.88 freetext 0.13 stopwords .82 ...\n",
    "\n",
    ".28 ...test type not chosen \n",
    "\n",
    ".98 stop...\n",
    "\n",
    ".97 stop...\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "\n",
    "This translates to a complete program:\n",
    "\n",
    "_____\n",
    "  \n",
    "\n",
    "ontology['test_clusterer_silouhette']\n",
    "\n",
    "(ontology['clusterer_sklearn_kmeans_20clusters']\n",
    "\n",
    "(vectorSpace\n",
    "\n",
    ")) \n",
    "\n",
    "(vectorSpace)\n",
    "\n",
    "vectorSpace=ontology['vectorSpace_gensim_doc2vec_size200_iterations1000_minfreq5']\n",
    "\n",
    "(data = ontology['preprocessor_freetext_emoji_removal']\n",
    "\n",
    "(data = ontology['preprocessor_freetext_lemmatization']\n",
    "\n",
    "(data = ontology['preprocessor_freetext_stopword']\n",
    "\n",
    "(data = ontology['data_freetext_internetResearchAgency']\n",
    "\n",
    "))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Python Program from an evolvable representation through Currying\n",
    "\n",
    "First we will demonstrate the python curry function, from https://mtomassoli.wordpress.com/2012/03/18/currying-in-python/ by Massimiliano Tomassoli, 2012, applied to normal nlp python programs, mentioned in the above example of an ontology and blackboard communication. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Massimiliano Tomassoli, 2012.\n",
    "#\n",
    "\n",
    "\n",
    "def genCur(func, unique = True, minArgs = None):\n",
    "    \"\"\" Generates a 'curried' version of a function. \"\"\"\n",
    "    def g(*myArgs, **myKwArgs):\n",
    "        def f(*args, **kwArgs):\n",
    "            if args or kwArgs:                  # some more args!\n",
    "                # Allocates data to assign to the next 'f'.\n",
    "                newArgs = myArgs + args\n",
    "                newKwArgs = dict.copy(myKwArgs)\n",
    "\n",
    "                # If unique is True, we don't want repeated keyword arguments.\n",
    "                if unique and not kwArgs.keys().isdisjoint(newKwArgs):\n",
    "                    raise ValueError(\"Repeated kw arg while unique = True\")\n",
    "\n",
    "                # Adds/updates keyword arguments.\n",
    "                newKwArgs.update(kwArgs)\n",
    "\n",
    "                # Checks whether it's time to evaluate func.\n",
    "                if minArgs is not None and minArgs <= len(newArgs) + len(newKwArgs):\n",
    "                    return func(*newArgs, **newKwArgs)  # time to evaluate func\n",
    "                else:\n",
    "                    return g(*newArgs, **newKwArgs)     # returns a new 'f'\n",
    "            else:                               # the evaluation was forced\n",
    "                return func(*myArgs, **myKwArgs)\n",
    "        return f\n",
    "    return g\n",
    "\n",
    "def cur(f, minArgs = None):\n",
    "    return genCur(f, True, minArgs)\n",
    "\n",
    "def curr(f, minArgs = None):\n",
    "    return genCur(f, False, minArgs)\n",
    "\n",
    "# Simple Function.\n",
    "def func(a, b, c, d, e, f, g = 100):\n",
    "    print(a, b, c, d, e, f, g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_clusterer_silhouette(X,Y):\n",
    "    \n",
    "    # \"_args\": [{  \"type\": \"numpy.ndarray\",  \"dtype\": \"float32\"},\n",
    "    #           {\"type\": \"numpy.ndarray\", \"dtype\": \"int32\"}],\n",
    "    # \"_return\": [{\"type\": \"float\"}]\n",
    "    \n",
    "    # we only want to test cosine metric for this example, but it could be a parameter in other cases\n",
    "    \n",
    "    import sklearn\n",
    "    from sklearn import metrics\n",
    "    print('test_clusterer_silhouette')\n",
    "    silhouette = metrics.silhouette_score(X, Y, metric = 'cosine')\n",
    "    return (silhouette)\n",
    "\n",
    "def test_clusterer_calinskiHarabaz(X,Y):\n",
    "    \n",
    "    # \"_args\": [{  \"type\": \"numpy.ndarray\",  \"dtype\": \"float32\"},\n",
    "    #           {\"type\": \"numpy.ndarray\", \"dtype\": \"int32\"}],\n",
    "    # \"_return\": [{\"type\": \"float\"}]\n",
    "    \n",
    "    # we only want to test cosine metric for this example, but it could be a parameter in other cases\n",
    "    \n",
    "    import sklearn\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    calinski_harabaz = metrics.calinski_harabaz_score(X, clusterAlgLabelAssignmentsSD) \n",
    "    return (calinski_harabaz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the NLP routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vectorSpace_gensim_doc2vec (X,size,iterations, minfreq):\n",
    "    \n",
    "    \n",
    "     #   \"_args\": [{\"type\": \"list\",\"firstElement\":\"gensim.models.doc2vec.TaggedDocument\" }],\n",
    "     #   \"_return\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\" }\n",
    "    \n",
    "    import gensim\n",
    "    import numpy as np\n",
    "    import sklearn.preprocessing\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    \n",
    "    print('vectorSpace_gensim_doc2vec')\n",
    "    \n",
    "    \n",
    "    model = gensim.models.doc2vec.Doc2Vec(size=size, min_count=minfreq,iter = iterations,dm=0)\n",
    "    model.build_vocab(X)\n",
    "    model.train(X, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    cmtVectors = [model.infer_vector(X[i].words) for i in range(len(X)) ]\n",
    "    cmtVectors = [inferred_vector for inferred_vector in cmtVectors \n",
    "                  if  not np.isnan(inferred_vector).any() \n",
    "                  and not np.isinf(inferred_vector).any()]\n",
    "   \n",
    "    X = StandardScaler().fit_transform(cmtVectors)\n",
    "    return(X)\n",
    "    \n",
    "def preprocessor_freetext_tag (X):\n",
    "    \n",
    "    #convert a list of strings to a tagged document\n",
    "    #if it is a list of a list of strings broadcast to a list of tagged documents\n",
    "\n",
    "\n",
    "    #   \"_args\": [{\"type\": \"list\",\"firstElement\":\"string\" }],\n",
    "    #   \"_return\": [{\"type\": \"list\",\"gensim.models.doc2vec.TaggedDocument\" }]\n",
    "    \n",
    "    import gensim\n",
    "    print ('preprocessor_freetext_tag')\n",
    "    \n",
    "    tag = lambda x,y: gensim.models.doc2vec.TaggedDocument(x,[y])\n",
    "    \n",
    "    if type(X) is str:\n",
    "        tagged = tag(X,X)\n",
    "    else:\n",
    "        tagged = [tag(x,y) for y,x in enumerate(X)]\n",
    "    return (tagged)\n",
    "    \n",
    "def preprocessor_freetext_lemmatization (X):\n",
    "    \n",
    "    #   \"_args\": [{\"type\": \"list\",\"firstElement\":\"string\" }],\n",
    "    #   \"_return\": [{\"type\": \"list\",\"firstElement\":\"list\" }]\n",
    "    \n",
    "    #converts string documents into list of tokens\n",
    "    #if given a list, broadcasts\n",
    "    import gensim\n",
    "    \n",
    "    print('preprocessor_freetext_lemmatization')\n",
    "    stopfile = 'stopwords.txt'\n",
    "    lemmatized = []\n",
    "    with open(stopfile,'r') as f:\n",
    "        stopwords = {word.lower().strip() for word in f.readlines()}\n",
    "        lemma = lambda x:[b.decode('utf-8') for b in gensim.utils.lemmatize(str(x),stopwords=frozenset(stopwords)) ]\n",
    "    \n",
    "        if type(X) is str:\n",
    "            lemmatized = lemma(X)\n",
    "        else:\n",
    "            lemmatized = [lemma(x) for x in X]\n",
    "    \n",
    "    return(lemmatized)\n",
    "    \n",
    "\n",
    "    \n",
    "def preprocessor_freetext_strip (X):\n",
    "    \n",
    "    # strips addresses and emojis. if you get string strip, if you get list broadcast\n",
    "    \n",
    "    #   \"_args\": [{\"type\": \"list\",\"firstElement\":\"string\" }],\n",
    "    #   \"_return\": [{\"type\": \"list\",\"firstElement\":\"string\" }]\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    print(\"preprocessor_freetext_strip\")\n",
    "    code ='utf-8'\n",
    "    strip = lambda  x: re.sub(r\"\\s?http\\S*\", \"\", x).encode(code).decode(code)\n",
    "    \n",
    "    #strip = lambda  x: re.sub(r\"\\s?http\\S*\", \"\", x).decode(code)\n",
    "    #strip = lambda  x: re.sub(r\"\\s?http\\S*\", \"\", x.decode(code))\n",
    "    #strip = lambda  x: re.sub(r\"\\s?http\\S*\", \"\", x)\n",
    "    \n",
    "    if type(X) is str:\n",
    "        decoded = strip(X)\n",
    "    else:\n",
    "        decoded = [strip(x) for x in X]\n",
    "    return (decoded)\n",
    "\n",
    "       \n",
    "def preprocessor_freetext_shuffle (X):\n",
    "    \n",
    "    #   \"_args\": [{\"type\": \"list\" }],\n",
    "    #   \"_return\": [{\"type\": \"list\" }]\n",
    "    import random\n",
    "    print(\"preprocessor_freetext_shuffle\")\n",
    "    random.shuffle(X)\n",
    "    return (X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_freetext_csvColumn(path, col = 'text'):\n",
    "    #  returns a list of documents that are strings\n",
    "    #   \"_return\": [{\"type\": \"list\",\"firstElement\":\"string\" }]\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    print('data_freetext_csvColumn_short')\n",
    "    raw_data = pd.read_csv(path, encoding = \"ISO-8859-1\")\n",
    "    docList = [raw_data.loc[i,col] for i in range (len(raw_data)) if raw_data.loc[i,col]]\n",
    "    return docList\n",
    "\n",
    "def data_vector_blobs(n_samples = 1500):\n",
    "    import sklearn\n",
    "    from sklearn.datasets import make_blobs\n",
    "    X,Y = make_blobs(n_samples=n_samples, random_state=8)\n",
    "    return X\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the clusterers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clusterer_sklearn_kmeans(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    \n",
    "    print ('clusterer_sklearn_kmeans')\n",
    "    clusterAlgSKN = MiniBatchKMeans(n_clusters).fit(X)\n",
    "    clusterAlgLabelAssignmentsSKN= clusterAlgSKN.predict(X)\n",
    "    return (clusterAlgLabelAssignmentsSKN)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_agglomerative(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    \n",
    "    average_linkage = AgglomerativeClustering(linkage=\"average\", \n",
    "        affinity=\"cosine\",n_clusters=params['n_clusters'], connectivity=connectivity).fit(X)\n",
    "    clusterAlgLabelAssignmentsSAG= average_linkage.labels_.astype(np.int)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSAG)\n",
    "\n",
    "def clusterer_sklearn_affinityPropagation(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import AffinityPropagation\n",
    "    \n",
    "    affinity_propagation = cluster.AffinityPropagation(damping=params['damping'], preference=params['preference']).fit(X)\n",
    "    clusterAlgLabelAssignmentsSAP= affinity_propagation.predict(X)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSAP)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_meanShift(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import MeanShift\n",
    "    \n",
    "    \n",
    "    bandwidth = sklearn.cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "    \n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True).fit(X)\n",
    "    clusterAlgLabelAssignmentsSM= ms.predict(X)\n",
    "        \n",
    "    return (clusterAlgLabelAssignmentsSM)\n",
    "\n",
    "def clusterer_sklearn_spectral(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    \n",
    "    spectral = SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"cosine\")\n",
    "    try:\n",
    "        clusterAlgLabelAssignmentsSS= None\n",
    "        spectral = spectral.fit(X)\n",
    "    except ValueError as e:\n",
    "        pass\n",
    "    else:\n",
    "        clusterAlgLabelAssignmentsSS= spectral.labels_.astype(np.int)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSS)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_ward(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "    ward = AgglomerativeClustering(n_clusters=params['n_clusters'], linkage='ward',\n",
    "                                   connectivity=connectivity).fit(X)\n",
    "    clusterAlgLabelAssignmentsSW= ward.labels_.astype(np.int)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSW)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_dbscan(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    \n",
    "    dbscan = DBSCAN(eps=params['eps']).fit(X)\n",
    "    clusterAlgLabelAssignmentsSD= dbscan.labels_.astype(np.int)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSD)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_birch(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn.cluster import Birch\n",
    "    \n",
    "    \n",
    "    birch = Birch(n_clusters=params['n_clusters']).fit(X)\n",
    "    clusterAlgLabelAssignmentsSB= birch.predict(X)\n",
    "        \n",
    "    return (clusterAlgLabelAssignmentsSB)\n",
    "\n",
    "\n",
    "def clusterer_sklearn_gaussian(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import sklearn\n",
    "    from sklearn import mixture\n",
    "    \n",
    "    clusterAlgSGN = mixture.GaussianMixture(n_components=params['n_clusters'], covariance_type='full').fit(X)\n",
    "    clusterAlgLabelAssignmentsSGN= clusterAlgSGN.predict(X)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsSGN)\n",
    "\n",
    "\n",
    "def clusterer_nltk_kmeans(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import nltk\n",
    "    from nltk.cluster.kmeans import KMeansClusterer\n",
    "    \n",
    "    \n",
    "    clusterAlgNK = KMeansClusterer(params['n_clusters'], distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters=True)\n",
    "    clusterAlgLabelAssignmentsNK = clusterAlgNK.cluster(cmtVectors, assign_clusters=True)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsNK)\n",
    "\n",
    "\n",
    "def clusterer_nltk_agglomerative(X, n_clusters):\n",
    "    \n",
    "     # \"_args\": [{\"type\": \"numpy.ndarray\",\"dtype\": \"float32\"} ],\n",
    "     #   \"_return\": [{ \"type\": \"numpy.ndarray\",\"dtype\": \"int32\"}\n",
    "\n",
    "    # in this case we want to try different numbers of clusters, so it is a parameter\n",
    "        \n",
    "    import nltk\n",
    "    from nltk.cluster.gaac import GAAClusterer\n",
    "    \n",
    "    \n",
    "    clusterAlgNG = GAAClusterer(num_clusters=params['n_clusters'], normalise=True, svd_dimensions=None)\n",
    "    clusterAlgLabelAssignmentsNG = clusterAlgNG.cluster(cmtVectors, assign_clusters=True)\n",
    "    \n",
    "    return (clusterAlgLabelAssignmentsNG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Fill the initial function\n",
    "ontology= {}\n",
    "ontology['data_freetext_csvColumn']= curr(data_freetext_csvColumn)\n",
    "ontology['data_vector_blobs']= curr(data_vector_blobs)\n",
    "ontology['preprocessor_freetext_shuffle'] = curr(preprocessor_freetext_shuffle)\n",
    "ontology['preprocessor_freetext_strip'] = curr(preprocessor_freetext_strip)\n",
    "ontology['preprocessor_freetext_lemmatization']  = curr(preprocessor_freetext_lemmatization)\n",
    "ontology['preprocessor_freetext_tag']= curr(preprocessor_freetext_tag)\n",
    "ontology['vectorSpace_gensim_doc2vec'] = curr(vectorSpace_gensim_doc2vec)\n",
    "ontology['clusterer_sklearn_kmeans'] = curr (clusterer_sklearn_kmeans)\n",
    "ontology['clusterer_sklearn_agglomerative'] = curr (clusterer_sklearn_agglomerative)\n",
    "ontology['clusterer_sklearn_affinityPropagation'] = curr (clusterer_sklearn_affinityPropagation)\n",
    "ontology['clusterer_sklearn_meanShift'] = curr (clusterer_sklearn_meanShift)\n",
    "ontology['clusterer_sklearn_spectral'] = curr (clusterer_sklearn_spectral)\n",
    "ontology['clusterer_sklearn_ward'] = curr (clusterer_sklearn_ward)\n",
    "ontology['clusterer_sklearn_dbscan'] = curr (clusterer_sklearn_dbscan)\n",
    "ontology['clusterer_sklearn_birch'] = curr (clusterer_sklearn_birch)\n",
    "ontology['clusterer_sklearn_gaussian'] = curr (clusterer_sklearn_gaussian)\n",
    "ontology['clusterer_nltk_agglomerative'] = curr (clusterer_nltk_agglomerative)\n",
    "ontology['clusterer_nltk_kmeans'] = curr (clusterer_nltk_kmeans)\n",
    "ontology['test_clusterer_silhouette']  = curr(test_clusterer_silhouette)\n",
    "ontology['test_clusterer_calinskiHarabaz']  = curr(test_clusterer_calinskiHarabaz)\n",
    "\n",
    "#Create the constructions that would be machine learned, using a shortened dataset.  These are just a few examples. The rest are \n",
    "#in the Registry.py file\n",
    "\n",
    "\n",
    "ontology['data_freetext_csvColumn_short']= ontology['data_freetext_csvColumn'](path = 'data/short.csv')\n",
    "ontology['clusterer_sklearn_kmeans_20clusters'] = ontology['clusterer_sklearn_kmeans'](n_clusters = 20)\n",
    "ontology['vectorSpace_gensim_doc2vec_size200_iterations1000_minfreq5']= ontology['vectorSpace_gensim_doc2vec'](size=200)(iterations = 1000)(minfreq = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First agent\n",
    "a = ontology['data_freetext_csvColumn_short']()\n",
    "b = ontology['preprocessor_freetext_shuffle'](a)()\n",
    "c = ontology['preprocessor_freetext_strip'](b)()\n",
    "d = ontology['preprocessor_freetext_lemmatization'](c)()\n",
    "e = ontology['preprocessor_freetext_tag'](d)()\n",
    "f = ontology['vectorSpace_gensim_doc2vec_size200_iterations1000_minfreq5'](e)()\n",
    "\n",
    "# Second agent\n",
    "g = ontology['clusterer_sklearn_kmeans_20clusters'](f)()\n",
    "h = ontology['test_clusterer_silhouette'] (f)(g)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all the parts of the NLP solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "c[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "e[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEP representation\n",
    "\n",
    "Once the individual python functions are parameterized, the remaining unbound input parameters are filled with calls to other python programs.  Once agents have finished constructing and buying programs, they have a sequential list, the input and output of which we must interpret.  We use the GEP, or Genetic Expressio Program representation.  Because this is a tree representation of general python programs that have different arities, it is disruptive in that a python program of different arity can change the meaning of all subsequent programs, and the farther away the argument list is, the more likely it is to be disrupted.  We stop at both a stop codon and when the input/output types do not match.  If we took the alternative of keeping meaning positional, then we would have to have consistent arity throughout, filling in spaces with nulls.  Although this would help convergence, it would take too much space. An althernative is, to not construct long programs but rather to trade tokens for them.  This creates good market conditions for trade, and encourages specialization, that is, agents that worth more money as they are repeatedly asked to do the same kind of problem.  From this we expect types to emerge, that are those specifically needed for certain applications.  These agents are expected to communicate their emergent type through the arbitrary sign.\n",
    "\n",
    "We start with a test program, that we know an answer to.  For a list of functions named with alphabetical letters in alphabetical order, GEP uses Karva notation, that would call them according to the illustrated tree (had they the arities listed in the arity map) That is, if root function a had arity two, then functions b and c would be its arguments, which are the next two values on the list.  The terminal functions are those with arity 0, which in our case return their names.  So if each of the functions returned what was sent up in order, and terminals sent their names, then GEP representation would print the result: \"pqklrso\".  We take out a portion of the simulation programs to clearly show how the representation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename='karva.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pickleThis(fn):  # define a decorator for a function \"fn\"\n",
    "    def wrapped(self, *args, **kwargs):   # define a wrapper that will finally call \"fn\" with all arguments    \n",
    "        cachefile = None\n",
    "        if args and args[0] in self.pickles:\n",
    "            pickle_name = self.pickles[args[0]]\n",
    "            cachefile = self.parameters['output_path']+ 'pickles/' + pickle_name\n",
    "\n",
    "\n",
    "        if cachefile and os.path.exists(cachefile):\n",
    "\n",
    "            with open(cachefile, 'rb') as cachehandle:\n",
    "                print(\"using pickled result from '%s'\" % cachefile)\n",
    "                return pickle.load(cachehandle)\n",
    "\n",
    "        # execute the function with all arguments passed\n",
    "        res = fn(self,*args, **kwargs)\n",
    "\n",
    "        pickle_name = str(self.pickle_count) + '.p'\n",
    "        self.pickle_count += 1\n",
    "        cachefile = self.parameters['output_path']+ 'pickles/' + pickle_name\n",
    "\n",
    "        # write to cache file\n",
    "        with open(cachefile, 'wb') as cachehandle:\n",
    "            pickle.dump(res, cachehandle)\n",
    "            self.pickles[args[0]] = pickle_name\n",
    "\n",
    "        return res\n",
    "\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from boltons.cacheutils import cachedmethod\n",
    "from boltons.cacheutils import LRU\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "class SnetSim_test(object):\n",
    "    \n",
    "    #todo:  implement hidden tests, because those that are marked hidden now can be seen on the blackboard by all.\n",
    "    # important because they constitute a hidden testing set as in kaggle\n",
    "    \n",
    "    def __init__(self, config_path,registry):\n",
    "              \n",
    "        with open(config_path) as json_file:  \n",
    "            config = json.load(json_file)\n",
    "        #print(json.dumps(config['ontology'], indent=2))\n",
    "        self.parameters = config['parameters']\n",
    "        self.blackboard = config['blackboard']\n",
    "        self.ontology = config['ontology']\n",
    "        self.registry = registry\n",
    "        pickle_config_path = config['parameters']['output_path']+ 'pickles/' +  'index.json'\n",
    "        with open(pickle_config_path) as json_file:  \n",
    "            pickle_config = json.load(json_file)\n",
    "        self.pickle_count = pickle_config['count'] #contains the next number for the pickle file\n",
    "        self.pickles = pickle_config['pickles']\n",
    "        \n",
    "        self.resultTuple = ()\n",
    "        #self.cache = LRU(max_size = 512)\n",
    "        self.cache = LRU()\n",
    "        \n",
    "    \n",
    "    @cachedmethod('cache')\n",
    "    @pickleThis\n",
    "    def memoisePickle(self,tupleKey):\n",
    "        if len(self.resultTuple):\n",
    "            result = self.registry[tupleKey[0]](*self.resultTuple)\n",
    "        else:\n",
    "            result = self.registry[tupleKey[0]]()\n",
    "        return (result)\n",
    "\n",
    "    def callMemoisePickle  (self,root):\n",
    "        #print ('In callMemoisePickle arg ' + root)\n",
    "        resultList = []\n",
    "        funcList = []\n",
    "        argTuple = ()\n",
    "        if root in self.gepResult:\n",
    "            args = self.gepResult[root]\n",
    "            argTuple = tuple(args)\n",
    "            for arg in args:\n",
    "                tfuncTuple, tresult = self.callMemoisePickle(arg)\n",
    "                resultList.append(tresult)\n",
    "                funcList.append(tfuncTuple)\n",
    "        carriedBack = tuple(funcList)\n",
    "        funcTuple = (root,carriedBack)\n",
    "        \n",
    "        self.resultTuple = tuple(resultList) #You have to set a global to memoise and pickle correctly\n",
    "              \n",
    "        result = self.memoisePickle(funcTuple)\n",
    "\n",
    "        return(funcTuple, result)  \n",
    "    \n",
    "     \n",
    "    def gep(self,functionListDontModify):\n",
    "        #assign input and output functions as defined by the Karva notation.  \n",
    "        #get arity of the items and divide the levels according to that arity, then make the assignments across the levels\n",
    "        #todo: take input / output compatability into account, skipping that which \n",
    "        \n",
    "        \n",
    "        #example.  for the following program list with the following arity, the karva notation result is the following \n",
    "        self.learnedProgram = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s']\n",
    "        self.arity = {'a':2,'b':3,'c':2,'d':2,'e':1,'f':1,'g':2,'h':1,'i':1,'j':1,'k':0,'l':0,'m':1,'n':1,'o':0,'p':0,'q':0,'r':0,'s':0}\n",
    "        \n",
    "        #this is what comes out of the gep function:  an assignment list of what functions form the parameters of the other\n",
    "        #functions based on arity.  It is calculated , but shown here for convenience.\n",
    "        self.results = {'a':['b','c'],'b':['d','e','f'],'c':['g','h'], 'd':['i','j'],'e':['k'],\n",
    "              'f':['l'],'g':['m','n'],'h':['o'],'i':['p'],'j':['q'],'m':['r'], 'n':['s']}\n",
    "        \n",
    "        #divide into levels\n",
    "        #dont modify the functionList\n",
    "        \n",
    "        functionList = []\n",
    "        functionList.extend(functionListDontModify)\n",
    "        \n",
    "        levels= {1:[functionList.pop(0)]}\n",
    "        \n",
    "        currentLevel = 1\n",
    "        #length_next_level = 0\n",
    "        maxiters = 100\n",
    "        count = 0\n",
    "        while functionList and count < maxiters:\n",
    "            count +=1\n",
    "            length_next_level = 0\n",
    "            for func in  levels[currentLevel]:\n",
    "                length_next_level += arity[func]\n",
    "            currentLevel += 1\n",
    "            levels[currentLevel]= functionList[0:length_next_level]\n",
    "            functionList = functionList[length_next_level:]\n",
    "            \n",
    "            \n",
    "        #make assignments\n",
    "        \n",
    "        gepResult = OrderedDict()\n",
    "        for level, functionList in levels.items():\n",
    "            next_level = level+1\n",
    "            cursor= 0\n",
    "            for func in functionList:\n",
    "                next_cursor = cursor + arity[func]\n",
    "                if next_level in levels:\n",
    "                    gepResult[func]= levels[next_level][cursor:next_cursor]\n",
    "                cursor = next_cursor\n",
    "                \n",
    "        return(gepResult)\n",
    "    \n",
    "    def performTest(self,functionList):\n",
    "        score = 0\n",
    "        #print (\"in perform test\")\n",
    "        self.gepResult = self.gep(functionList) #put the ordered Dictionary in the global so a decorated function can access\n",
    "        if any(self.gepResult.values()):\n",
    "            root = next(iter(self.gepResult.items()))[0]\n",
    "            score = self.callMemoisePickle  (root)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call with Gep, memoise, and pickle simultaneously. A Boltons is used for memoising, while we wrote our own decorator function for pickling.  These decorator function take a tuple tree as the input, that has a one to one correspondance with an arrangeemnt of prgrams, and then the answer. For the answer we see, first , a tuple tree and then the answer that we expected.  The tuple tree uniquely designates the order of functions . Since tuples are hasable in python, this representation, for the curried functions, enables us to both pickle and memoise the exact function call set so that not only each result, but each parital result need never be called again.  By memoise we need that a specific amount of RAM is set aside for results and within are kept the most recent computations, as is needed in many machine learning programs.  In every case, the pickles are also saved to the disk for subsequent runs of the same scenario.  First we will look at the memoising stats, that show that the cache was hit the second time that the same program was called.  Then we look at the pickles made during the simulation intialization and short run above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "snetsim = SnetSim_test('config_test.json', test)    \n",
    "snetsim.performTest(learnedProgram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print((snetsim.cache.hit_count, snetsim.cache.miss_count, snetsim.cache.soft_miss_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "snetsim.performTest(learnedProgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print((snetsim.cache.hit_count, snetsim.cache.miss_count, snetsim.cache.soft_miss_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that not only the result is stored, but important intermediates which will help speed up combinitorial optimization, we look in the pickle index that was saved to directory when ten iterations of the simulation were run above.  The pickles are named with  a number and a .p extension.  The index maps the program order to the pickle name in the pickled directory. When the simuation is run again, the pickles are reloaded.  We see that 10 pickles have been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "pickled = \"competing_clusterers/pickles/index.p\"\n",
    "if os.path.exists(pickled):\n",
    "    with open(pickled, 'rb') as cachehandle:\n",
    "        pickle_index =  pickle.load(cachehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pickle_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simulation Loop\n",
    "\n",
    "The simulation consists of a registry of programs with which the machine learning agents compose solutions by parameterizing and ordering them, an ontology that describes those programs from general to specific, a SnetSim agent that takes care of global things like the caches and calling the staged activation of the agents and finally an SnetAgent that has all the routines to select partners, which the user subclasses to implement their own machine learning / reinforcment learning algorithms.  \n",
    "\n",
    "Users submit agents that can perform two routines, the step routine that puts a message on the blackboard, and a payment_notification routine that the user can write to keep track of which trades they are paid for. If the user submits a machine learning / reinforcement learning algorithm, then it would submit a message that would optimize a quality, such as quantity of AGI tokens.  In the simulation, every agent at random puts their message on the blackboard. An agents only job is to put their list of programs they will buy sell and construct, as well as the terms for trade, on the blackboard. The agent gets response from its message on the blackboard not immediately, but before its next message is due.  So instead of step an response as in open ai gym, the singularity net simulation does response (from the last message) and then step.  The simulation calls the step, so that all agents have time to move before a response is received.  After all agents step, for every buy on the blackboard, the simulation ranks those with overlaping prices and items of the correct categories by the cosine distance of the sign the buyer seeks to the sign that the seller is displaying.  Selection is done by roulette wheel where the farthest sign of agents that have correspending trade plans has a zero percent chance of being chosen.  After trades are made, each agent has a list of programs in a row, that will be interpreted by a call to GEP.  Tests are then run if required, and if the agent is a human, funds are distributed.  The agents are notified of change of funds at the time their particular trade is part of a solution that a human buys, but can also see all the messages, who won trades and money, and how well each did on every test, on the blackboard.  \n",
    "\n",
    "This may be a centralized market for the current settings of the program, but what the agent sees is easily modified with a mesa network, including being able to see only its neighbors and having to pay a price for hopping to more distant neighobors, as one might expect in a blockchain network.\n",
    "\n",
    "The same configuration file is written to in the logs, including the ofers that each agent make in a buy, the similarity of the agents, what agents were chosen,their price and their scores on each test.  We take a log from the ninth iteration of the simulation that was run earlier in this notebook. In this run the human, the first agent, has gained many pieces of information since the last message submission, including sign similarities and probabilities of being accepted, test scores, and prices.  By examining signs of agents with plans that correspond, the sign the first offerrer displayed made him have about 75% chance of being selected, while the sign the third offerrer displayed gave him a 25% chance of selection, but the less prefered still won, possibly because the more preferred agent did not purchase a specialist vector space (as he had in other stochastic runs) .  Scores are lower then they would be for the software because we are using a small data set here, just to demonstrte the functionality of the simulatin package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "log = 'competing_clusterers/logs/log9.000000000000021.txt'\n",
    "with open(log) as json_file:  \n",
    "    config = json.load(json_file)\n",
    "    print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part we have examined a general, flexible, and evolvable representation for the agent communication and for the ontology.  We have seen how a python program may be generated from agent communications about buying, selling, and constructing software.  This representation can make use of statistics about the frequency of functions occurring in problem types, such as in Microsoft's Deep coder.  We have seen all the lower level details of how the simulation works, but do not yet see the patterns that these designs create, or how such a simple design can faciliate agent self organization and growth, or an agent economy with a natural price.  In part two we will address a coevolutionary representation that leverages a rich heterogeneous environment to make the evolution of python programs within reach, to make clear the reasoning behind our design decisions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:simulation]",
   "language": "python",
   "name": "conda-env-simulation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
